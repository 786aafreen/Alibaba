{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyObnz8R4YUzltUilrgENhQL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/786aafreen/Alibaba/blob/main/Data_Anal_Assesment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s_z3uy8IrgHd"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as stats\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set() #setting the default seaborn style for our plots"
      ],
      "metadata": {
        "id": "DOyiYV9Uss6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "77ryJlF8sy9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q.1 . For the given insurance dataset justify the hypothesis\n",
        "1. Does bmi of males differ significantly from that of females?\n",
        "2. Is the proportion of smokers significantly different in different genders?"
      ],
      "metadata": {
        "id": "TfgN4vGks7yH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/insurance (1).csv') # read the data as a data frame"
      ],
      "metadata": {
        "id": "jB5Jf32GtBqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "az_ViqGAtbTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic EDA we have to here ::\n",
        "1. First find the shape of the data,data type of individual columns.\n",
        "\n",
        "2. Then check the null values.\n",
        "\n",
        "3. Now Descriptive stats of numerical columns.\n",
        "\n",
        "4. And then find out the distribution of numerical columns and the asssociated skewness and presence of outliers. \n",
        "\n",
        "5. Then need to find Distribution of categorical columns."
      ],
      "metadata": {
        "id": "W_Nnde2FtkHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "SAahfEMatdTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The data has 1338 instances with 7 attributes, 2 integer type, 2 float type and 3 object type(Strings in the column)."
      ],
      "metadata": {
        "id": "YryxLfrguZih"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check the null values"
      ],
      "metadata": {
        "id": "xF4FCWSaulj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().apply(pd.value_counts)"
      ],
      "metadata": {
        "id": "254fTlnYufWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are no null values in any of the column\n"
      ],
      "metadata": {
        "id": "RX86Jy9FuyxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# five point summary of the continuous attributes"
      ],
      "metadata": {
        "id": "fyBklMMCu-t3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().T"
      ],
      "metadata": {
        "id": "yp0UWEGxuutS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Data looks legit as all the statistics seem reasonable\n",
        "- Looking at the age column, data looks representative of the true age distribution of the adult population\n",
        "- Very few people have more than 2 children. 75% of the people have 2 or less children\n",
        "- The claimed amount is higly skewed as most people would require basic medi-care and only few suffer from diseases which cost more to get rid of"
      ],
      "metadata": {
        "id": "PzPDDuogvOEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plots to see the distribution of the continuous features individually\n",
        "\n",
        "plt.figure(figsize= (20,15))\n",
        "plt.subplot(3,3,1)\n",
        "plt.hist(df.bmi, color='blue', edgecolor = 'black', alpha = 0.7)\n",
        "plt.xlabel('bmi')\n",
        "\n",
        "plt.subplot(3,3,2)\n",
        "plt.hist(df.age, color='green', edgecolor = 'black', alpha = 0.7)\n",
        "plt.xlabel('age')\n",
        "\n",
        "plt.subplot(3,3,3)\n",
        "plt.hist(df.charges, color='orange', edgecolor = 'black', alpha = 0.7)\n",
        "plt.xlabel('charges')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "evqAun1DvOxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- bmi looks quiet normally distributed.\n",
        "\n",
        "- Age seems be be distributed quiet uniformly.\n",
        "\n",
        "- As seen in the previous step, charges are highly skewed."
      ],
      "metadata": {
        "id": "Ym-F7lZxv_w-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Skewness = pd.DataFrame({'Skewness' : [stats.skew(df.bmi),stats.skew(df.age),stats.skew(df.charges)]},\n",
        "                        index=['bmi','age','charges'])  # Measure the skeweness of the required columns\n",
        "Skewness"
      ],
      "metadata": {
        "id": "mG1tIOdHwGe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Skew of bmi is very less as seen in the previous step\n",
        "- age is uniformly distributed and there's hardly any skew\n",
        "- charges are highly skewed"
      ],
      "metadata": {
        "id": "0wETu-4ZwVf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize= (20,15))\n",
        "plt.subplot(3,1,1)\n",
        "sns.boxplot(x= df.bmi, color='pink')\n",
        "\n",
        "plt.subplot(3,1,2)\n",
        "sns.boxplot(x= df.age, color='red')\n",
        "\n",
        "plt.subplot(3,1,3)\n",
        "sns.boxplot(x= df.charges, color='chocolate')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "018ks-NPwWIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- bmi has a few extreme values\n",
        "- charges as it is highly skewed, there are quiet a lot of extreme values"
      ],
      "metadata": {
        "id": "AGTajZS8xW6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(20,25))\n",
        "\n",
        "\n",
        "x = df.smoker.value_counts().index    #Values for x-axis\n",
        "y = [df['smoker'].value_counts()[i] for i in x]   # Count of each class on y-axis\n",
        "\n",
        "plt.subplot(4,2,1)\n",
        "plt.bar(x,y, align='center',color = 'red',edgecolor = 'black',alpha = 0.7)  #plot a bar chart\n",
        "plt.xlabel('Smoker?')\n",
        "plt.ylabel('Count ')\n",
        "\n",
        "plt.title('Smoker distribution')\n",
        "\n",
        "x1 = df.sex.value_counts().index    #Values for x-axis\n",
        "y1 = [df['sex'].value_counts()[j] for j in x1]   # Count of each class on y-axis\n",
        "\n",
        "plt.subplot(4,2,2)\n",
        "plt.bar(x1,y1, align='center',color = 'gold',edgecolor = 'black',alpha = 0.7)  #plot a bar chart\n",
        "plt.xlabel('Gender')\n",
        "\n",
        "plt.ylabel('Count')\n",
        "plt.title('Gender distribution')\n",
        "\n",
        "x2 = df.region.value_counts().index    #Values for x-axis\n",
        "y2 = [df['region'].value_counts()[k] for k in x2]   # Count of each class on y-axis\n",
        "\n",
        "plt.subplot(4,2,3)\n",
        "plt.bar(x2,y2, align='center',color = 'yellow',edgecolor = 'black',alpha = 0.7)  #plot a bar chart\n",
        "plt.xlabel('Region')\n",
        "plt.ylabel('Count ')\n",
        "plt.title(\"Regions' distribution\")\n",
        "\n",
        "x3 = df.children.value_counts().index    #Values for x-axis\n",
        "y3 = [df['children'].value_counts()[l] for l in x3]   # Count of each class on y-axis\n",
        "\n",
        "plt.subplot(4,2,4)\n",
        "plt.bar(x3,y3, align='center',color = 'lightblue',edgecolor = 'black',alpha = 0.7)  #plot a bar chart\n",
        "plt.xlabel('No. of children')\n",
        "plt.ylabel('Count ')\n",
        "plt.title(\"Children distribution\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ILfs5DEQxa54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- There are a lot more non-smokers than there are smokers in the data\n",
        "- Instances are distributed evenly accross all regions\n",
        "- Gender is also distributed evenly\n",
        "- Most instances have less than 2 children and very few have 4 or 5 children"
      ],
      "metadata": {
        "id": "xCmiAXJuxxi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Bi-variate distribution of every possible attribute pair"
      ],
      "metadata": {
        "id": "sCLUBzX_x7mX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Label encoding the variables before doing a pairplot because pairplot ignores strings\n",
        "df_encoded = copy.deepcopy(df)\n",
        "df_encoded.loc[:,['sex', 'smoker', 'region']] = df_encoded.loc[:,['sex', 'smoker', 'region']].apply(LabelEncoder().fit_transform) \n",
        "\n",
        "sns.pairplot(df_encoded)  #pairplot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xXcbNGcYx-Da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The only obvious correlation of 'charges' is with 'smoker'\n",
        "- Looks like smokers claimed more money than non-smokers\n",
        "- There's an interesting pattern between 'age' and 'charges. Could be because for the same ailment, older people are charged more than the younger ones"
      ],
      "metadata": {
        "id": "XIl-nVvAyHL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Does bmi of males differ significantly from that of females?"
      ],
      "metadata": {
        "id": "ag49MfVOyPFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.sex.value_counts()"
      ],
      "metadata": {
        "id": "U3G8ckAnyTXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.scatterplot(df.age, df.charges,hue=df.sex,palette= ['pink','lightblue'] )\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "zo27ZMpPxStY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Visually, there is no apparent relation between gender and charges"
      ],
      "metadata": {
        "id": "FXyTKD_vyiRC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# T-test to check dependency of bmi on gender\n",
        "Ho = \"Gender has no effect on bmi\"   # Stating the Null Hypothesis\n",
        "Ha = \"Gender has an effect on bmi\"   # Stating the Alternate Hypothesis\n",
        "\n",
        "x = np.array(df[df.sex == 'male'].bmi)  # Selecting bmi values corresponding to males as an array\n",
        "y = np.array(df[df.sex == 'female'].bmi) # Selecting bmi values corresponding to females as an array\n",
        "\n",
        "t, p_value  = stats.ttest_ind(x,y, axis = 0)  #Performing an Independent t-test\n",
        "\n",
        "if p_value < 0.05:  # Setting our significance level at 5%\n",
        "    print(f'{Ha} as the p_value ({p_value.round()}) < 0.05')\n",
        "else:\n",
        "    print(f'{Ho} as the p_value ({p_value.round(3)}) > 0.05')"
      ],
      "metadata": {
        "id": "lsSmU-xaynyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Gender has no effect on bmi as the p_value (0.09) > 0.05"
      ],
      "metadata": {
        "id": "j0JjvuJwyv8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#bmi of both the genders are identical"
      ],
      "metadata": {
        "id": "fdfZ2Kauy0sp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q.2 . Create portfolio for the given stocks:  &#39;GLD&#39;, &#39;AMZN\n",
        "      for the last 10 years.\n",
        "2. Visualize the expected returns on the 10 years series.\n",
        "3. Evaluate the annual daily mean, correlation,   Sharpe ratio and daily standard mean.\n",
        "4. . Discuss on the optimal portfolio and the different parameters evaluated for the portfolio."
      ],
      "metadata": {
        "id": "_Kb2oL6o1btw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Sharpe ratio is the average return earned in excess of the risk-free rate per unit of volatility (in the stock market, volatility represents the risk of an asset)"
      ],
      "metadata": {
        "id": "mOXprfY51hNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install yfinance package. \n",
        "!pip install yfinance "
      ],
      "metadata": {
        "id": "4T-_uKJ9ytUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf"
      ],
      "metadata": {
        "id": "5zSLCLSV6l87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create portfolio for the given stocks: GLD, amazon for the last 10 years.\n"
      ],
      "metadata": {
        "id": "iPDEYnzp6ndF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fetching the data from stocks of GLD and Amazon via specifying stock ticker"
      ],
      "metadata": {
        "id": "FLKHRFTx62aP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GLD = yf.download('GLD','2012-12-12','2022-12-12')\n",
        "AMZN =yf.download('AMZN','2012-12-12','2022-12-12')"
      ],
      "metadata": {
        "id": "KkXKgZps6myF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the Close Prices"
      ],
      "metadata": {
        "id": "ozbRUmM_7GGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AMZN.Close.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WysQujy07D0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLD.Close.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j9VS5WUS7JlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLD.tail()"
      ],
      "metadata": {
        "id": "zjnm5WSn7QC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AMZN.drop(['Open','High','Low','Close','Volume'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "Wgld6VRL7eof"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLD.drop(['Open','High','Low','Close','Volume'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "RmFUz7a67gCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Normalize the Prices"
      ],
      "metadata": {
        "id": "PQRCQDR27qCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GLD.iloc[0]['Adj Close']"
      ],
      "metadata": {
        "id": "sJA8rytW7v1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for stock_df in(GLD,AMZN):\n",
        "  stock_df['Normed Return']=stock_df['Adj Close']/stock_df.iloc[0]['Adj Close']"
      ],
      "metadata": {
        "id": "3DTlOfj373ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AMZN.isnull().sum()"
      ],
      "metadata": {
        "id": "dHxcxBS98BkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GLD.isnull().sum()"
      ],
      "metadata": {
        "id": "k0tBpwK38Cg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Allocations\n",
        "Now Let us profess that here are some allocations below for total portfolio:\n",
        "\n",
        "* 65% in GLD\n",
        "* 35% in Amazon\n",
        "\n",
        "So these values must be reflected by multiplying our Normed Return via outer Allocations"
      ],
      "metadata": {
        "id": "YGAKPySb8XOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for stock_df,allo in zip([GLD,AMZN],[0.65,0.35]):\n",
        "  stock_df['Allocation']=stock_df['Normed Return']*allo"
      ],
      "metadata": {
        "id": "OHuFNHEW8o1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Investment Let's profess we invested a million dollars in this portfolio"
      ],
      "metadata": {
        "id": "Te692fv49FJ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for stock_df in (GLD,AMZN):\n",
        "    stock_df['Position Values']=stock_df['Allocation']*1000000"
      ],
      "metadata": {
        "id": "VqKjRjHv9FrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val = pd.concat([GLD['Position Values'],AMZN['Position Values']],axis=1)"
      ],
      "metadata": {
        "id": "HMCILRhP9We3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val.head()"
      ],
      "metadata": {
        "id": "TsPsSWjn9gLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val.columns = ['GLD_Pos','AMZN_Pos']"
      ],
      "metadata": {
        "id": "Ul01WTWJ9j8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val.tail()"
      ],
      "metadata": {
        "id": "LehXHAfz9qVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val['Total_Pos'] = portfolio_val.sum(axis=1)"
      ],
      "metadata": {
        "id": "JHeilEqS9vIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val.tail()"
      ],
      "metadata": {
        "id": "2VPcIcxv9x2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Daily Return"
      ],
      "metadata": {
        "id": "G4qHt0Nbxvnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val['Total_Pos'].pct_change(1)"
      ],
      "metadata": {
        "id": "0NP81Ji9xwB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val['Daily Returns']=portfolio_val['Total_Pos'].pct_change(1)\n",
        "portfolio_val"
      ],
      "metadata": {
        "id": "cIrP6dpvx2fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the expected returns on the 10 years series."
      ],
      "metadata": {
        "id": "SFVePhlSyOfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val['Daily Returns'].plot(kind='kde')"
      ],
      "metadata": {
        "id": "AkPa_4unx5H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AMZN['Adj Close'].pct_change(1).plot(kind='kde')\n",
        "GLD['Adj Close'].pct_change(1).plot(kind='kde')"
      ],
      "metadata": {
        "id": "FnRRHVL1x9VU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discuss on the optimal portfolio and the different parameters evaluated for the portfolio.(Assuming Greater Return of investment)"
      ],
      "metadata": {
        "id": "QgCYuEFHyEHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us assume our expected returns"
      ],
      "metadata": {
        "id": "MxJClXJzyajS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ExpectedReturn = [9.00,9.20,9.40,9.60,9.80,10.00,10.20,10.40,10.60,10.80,11.00]\n",
        "StandardDeviation = [8.0,7.5,7.1,6.9,6.8,7.0,7.3,7.8,8.5,9.2,10.0]"
      ],
      "metadata": {
        "id": "3wazzw8zyeA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(StandardDeviation,ExpectedReturn,color='red')\n",
        "plt.xlabel(\"StandardDeviation (in %)\",color='Chocolate')\n",
        "plt.ylabel(\"ExpectedReturn (in %)\",color='brown')\n",
        "plt.title(\"Markowitz Portfolio Analysis\",color='Purple')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IysB_ceFyjXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val['Total_Pos'].plot(figsize=(8,8),color='red')\n",
        "plt.title('Total Portfolio Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "261vJDoiymL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "portfolio_val.drop('Total_Pos',axis=1).plot(kind='line')"
      ],
      "metadata": {
        "id": "qPy3_X3yyopN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sharpe Ratio"
      ],
      "metadata": {
        "id": "2YHIpHk_yrO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SR=portfolio_val['Daily Returns'].mean()/portfolio_val['Daily Returns'].std()\n",
        "SR"
      ],
      "metadata": {
        "id": "rTew0eIvyxU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ASR=(252**0.5)*SR\n",
        "ASR"
      ],
      "metadata": {
        "id": "veST_gZSy1Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AMZN['Adj Close'].pct_change(1).plot(kind='kde')\n",
        "GLD['Adj Close'].pct_change(1).plot(kind='kde')"
      ],
      "metadata": {
        "id": "0jfGnGhry5D0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "correlation"
      ],
      "metadata": {
        "id": "lyx3bJ6VzF39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data=pd.concat([GLD['Adj Close'],AMZN['Adj Close']],axis=1)"
      ],
      "metadata": {
        "id": "xbgqhIxUzGhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data.head()"
      ],
      "metadata": {
        "id": "_qL55cRezPeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(stock_data/stock_data.iloc[0] * 100).plot(figsize = (10,8))"
      ],
      "metadata": {
        "id": "x2sGnunOzSTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logReturns = np.log(stock_data/stock_data.shift(1))\n",
        "logReturns"
      ],
      "metadata": {
        "id": "SL67oRq9zVCs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the annual daily mean, correlation, Sharpe ratio and daily standard mean."
      ],
      "metadata": {
        "id": "I1WL8Wg2zav2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To obtain annual average returns!\n",
        "logReturns.mean() * 250"
      ],
      "metadata": {
        "id": "FC0ICs6JzbU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To obtain annual covariance between PG and Microsoft\n",
        "logReturns.cov() * 250 "
      ],
      "metadata": {
        "id": "-rt_2HX5zgZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stock_data.corr()"
      ],
      "metadata": {
        "id": "-t2_bUVzzii8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "99TkgY4rzkuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(stock_data.corr())\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e4y3FafvznAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stocks = ['GLD', 'AMZN']"
      ],
      "metadata": {
        "id": "__7uc5s2zpi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamically generating weights code"
      ],
      "metadata": {
        "id": "fOTowJ7QzxiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numberOfStocks = len(stocks)\n",
        "numberOfStocks"
      ],
      "metadata": {
        "id": "JzG2a8RazzgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed()\n",
        "weights_1 = np.random.random(numberOfStocks)\n",
        "weights_1"
      ],
      "metadata": {
        "id": "QfhrhIbvz2AH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weights = weights_1/np.sum(weights_1)\n",
        "weights"
      ],
      "metadata": {
        "id": "wY89-BHGz4L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now Calculating expected return of portfolio"
      ],
      "metadata": {
        "id": "GRAeo34Fz60d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(weights * logReturns.mean()).sum() * 250"
      ],
      "metadata": {
        "id": "tiRYlDw3z7S3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Expected Standard Deviation and volatility"
      ],
      "metadata": {
        "id": "4sKvXxNJ0AVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.sqrt(np.dot(weights.T,np.dot(logReturns.cov() * 250, weights)))"
      ],
      "metadata": {
        "id": "Wt_Tz2p-0AzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expectedReturn = []\n",
        "standardDeviation = []\n",
        "weightList0 = []\n",
        "weightList1 = []\n",
        "\n",
        "\n",
        "# Running simulations for finding optimum weights\n",
        "for i in range(120):\n",
        "    weights = np.random.random(numberOfStocks)\n",
        "    weights = weights/ weights.sum()\n",
        "    weightList0.append(weights[0])\n",
        "    weightList1.append(weights[1])\n",
        "    \n",
        "    expectedReturn.append((weights * logReturns.mean()).sum() * 250)\n",
        "    standardDeviation.append(np.sqrt(np.dot(weights.T, np.dot(logReturns.cov() * 250, weights))))\n",
        "\n",
        "#Converting lists into arrays\n",
        "\n",
        "weightList1 = np.array(weightList1)  #Weights for AMZN\n",
        "weightList1 = np.array(weightList1) #Weights for GLD\n",
        "expectedReturn = np.array(expectedReturn) \n",
        "standardDeviation = np.array(standardDeviation)\n",
        "\n",
        "#Creating dataframe\n",
        "df = pd.DataFrame({\"Weight of GLD\": weightList0, \"Weight of AMZN\": weightList1, \"Expected Return\": expectedReturn, \"Standard deviation\": standardDeviation})\n",
        "df.head()"
      ],
      "metadata": {
        "id": "lHCoMdR20Ihe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(14, 10), dpi=80)\n",
        "plt.scatter(df[\"Standard deviation\"], df[\"Expected Return\"])\n",
        "plt.xlabel(\"Standard deviation\")\n",
        "plt.ylabel(\"Expected return (in %)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pez2Y6uR0myM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df[\"Expected Return\"]>0.20) & (df[\"Expected Return\"]< 0.23)].sort_values(by=['Expected Return'])"
      ],
      "metadata": {
        "id": "JYlMMwiL0ril"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df[\"Expected Return\"]>0.20)].sort_values(by=['Expected Return']).head(10)"
      ],
      "metadata": {
        "id": "IoLt9CEP0xJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Expected Return\"].mean()"
      ],
      "metadata": {
        "id": "-A7AsOdN0zc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Expected Return\"].sort_values().median()"
      ],
      "metadata": {
        "id": "D193TF4N01gn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[(df[\"Expected Return\"]>0.25)].sort_values(by=['Expected Return'])"
      ],
      "metadata": {
        "id": "7AMpX-SO03Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[56]"
      ],
      "metadata": {
        "id": "3CaJ2AlZ05YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[21]"
      ],
      "metadata": {
        "id": "oJdMNEfy06UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q.3 For the dataset that includes CO2 emissions from each energy resource starting January 1973\n",
        "to July 2016. Answer the following\n",
        "\n",
        "1. Check the stationary using ADF test and autocorrelation plot.\n",
        "2. Forecast the target variable prediction using a suitable type of model.\n",
        "3. Evaluate the different types of error residues to check fitness of good of the model.\n",
        "4. Forecast the predictions for next 10 years on target variable."
      ],
      "metadata": {
        "id": "RlqGfpnx9zo9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pylab\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pylab import rcParams\n",
        "rcParams['figure.figsize'] = 20, 16"
      ],
      "metadata": {
        "id": "8xYjIB-s95LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import itertools\n",
        "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages"
      ],
      "metadata": {
        "id": "vXk0r6na-LA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time series dataset retrieving and visualization"
      ],
      "metadata": {
        "id": "AJNrfDIm-OPe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- First, in the following cells, we will retrive the monthly CO2 emissions dataset then we will visualize the dataset to decide the type of model we will use to model and analyse our time series (ts)."
      ],
      "metadata": {
        "id": "E2AIsOSq-TtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"/content/drive/MyDrive/co2.csv\")\n",
        "df1 .head()"
      ],
      "metadata": {
        "id": "7tgfGZRbRPzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.info()"
      ],
      "metadata": {
        "id": "qbuVPWcGBFbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has 6 columns where 2 of them are integer data type and 4 objects and 5094 observations."
      ],
      "metadata": {
        "id": "t9x-r0NZBN9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To read the dataset as a time series, we have to pass special arguments to the read_csv command as given below."
      ],
      "metadata": {
        "id": "TR5uVCKBBadP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dateparse = lambda x: pd.to_datetime(x, format='%Y%m', errors = 'coerce')\n",
        "df1 = pd.read_csv(\"/content/drive/MyDrive/co2.csv\", parse_dates=['YYYYMM'], index_col='YYYYMM', date_parser=dateparse) \n",
        "df1.head()"
      ],
      "metadata": {
        "id": "m3R-fiUbBOqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The arguments can be explained::\n",
        "\n",
        "- parse_dates: This is a key to identify the date time column. Example, the column name is ‘YYYYMM’.\n",
        "- index_col: This is a key that forces pandas to use the date time column as index.\n",
        "- date_parser: Converts an input string into datetime variable."
      ],
      "metadata": {
        "id": "oZ7SY8P0Bq4v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1.tail(15)"
      ],
      "metadata": {
        "id": "8VH0sCtkBxn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.Column_Order.value_counts()"
      ],
      "metadata": {
        "id": "7yo2e5tCCGG_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# df1.Unit.value_counts()      Million Metric Tons of Carbon Dioxide    5094\n",
        "#df1.Description.value_counts() values\n",
        "#df1.MSN.value_counts() value"
      ],
      "metadata": {
        "id": "qxggjJySCOgv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Total sum of CO2 emission from each energy group for every year is given as an observation that can be viewed in the NaT row. So, let us first identify and drop the non datetimeindex rows and also use ts to refere the time series dataset instead of the dataframe df. First, let us convert the index to datetime, coerce errors, and filter NaT"
      ],
      "metadata": {
        "id": "_CrCqmp2CevB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts = df1[pd.Series(pd.to_datetime(df1.index, errors='coerce')).notnull().values]\n",
        "ts.head(15)"
      ],
      "metadata": {
        "id": "xsOBfmG1B2zB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts.dtypes"
      ],
      "metadata": {
        "id": "EpIN8sT2CtRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the ts data type, the emission value is represented as an object. Let us first convert the emision value into numeric value as follows"
      ],
      "metadata": {
        "id": "hCw1rNCACxrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ss = ts.copy(deep=True)\n",
        "ts['Value'] = pd.to_numeric(ts['Value'] , errors='coerce')\n",
        "ts.head()"
      ],
      "metadata": {
        "id": "m9zEAEZRCyDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ts.info()"
      ],
      "metadata": {
        "id": "uuPk9DjIC4L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4323 observations have emissions value and therefore, we need to drop the empty rows emissions value."
      ],
      "metadata": {
        "id": "mOVM0ldbC754"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts.dropna(inplace = True)"
      ],
      "metadata": {
        "id": "efKel1TqC_k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Time series dataset visualization"
      ],
      "metadata": {
        "id": "Br_Qf8PkDEBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has 8 energy sources of CO2 emission. In the following cell, we will group the CO2 Emission dataset based on the type of energy source."
      ],
      "metadata": {
        "id": "kJmeU0UUDKa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#group by products same products changing date(month)\n",
        "Energy_sources = ts.groupby('Description')\n",
        "Energy_sources.head()"
      ],
      "metadata": {
        "id": "P5jvMrDWRs3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CO2 emission time series dataset is ploted to visualize the dependency of the emission in the power generation with time."
      ],
      "metadata": {
        "id": "e-l0_I41DTJf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CO2_per_source = ts.groupby('Description')['Value'].sum().sort_values()"
      ],
      "metadata": {
        "id": "OpeU68AvSH1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to use shorter descriptions for the energy sources\n",
        "CO2_per_source.index"
      ],
      "metadata": {
        "id": "Orco4ar_SL8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = ['Geothermal Energy', 'Non-Biomass Waste', 'Petroleum Coke','Distillate Fuel ',\n",
        "        'Residual Fuel Oil', 'Petroleum', 'Natural Gas', 'Coal', 'Total Emissions']"
      ],
      "metadata": {
        "id": "u-5APB1ESPoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (16,9))\n",
        "x_label = cols\n",
        "x_tick = np.arange(len(cols))\n",
        "plt.bar(x_tick, CO2_per_source, align = 'center', alpha = 0.5)\n",
        "fig.suptitle(\"CO2 Emissions by Electric Power Sector\", fontsize= 25)\n",
        "plt.xticks(x_tick, x_label, rotation = 70, fontsize = 20)\n",
        "plt.yticks(fontsize = 20)\n",
        "plt.xlabel('Carbon Emissions in MMT', fontsize = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1nRJrTE4ST3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1.plot(figsize=(8,5))\n",
        "plt.title('Monthly CO2 Concentration (ppm)')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bH2l1RH2DTlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the bar chart, we can see that the contribution of coal to the total CO2 emission is significant followed by natural gas."
      ],
      "metadata": {
        "id": "ac5YI6DtFc3w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural gas CO2 emission analysis\n",
        "\n",
        "For developing the time series model and make forcasting, I will use the natural gas CO2 emission from the electirical power generetion. First, let us slice this data from the ts as follows:"
      ],
      "metadata": {
        "id": "3pAPmA2oFPtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts.head()"
      ],
      "metadata": {
        "id": "jUy8_ixySeK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forecasting** is a technique that uses historical data as inputs to make informed estimates that are predictive in determining the direction of future trends. Businesses utilize forecasting to determine how to allocate their budgets or plan for anticipated expenses for an upcoming period of time."
      ],
      "metadata": {
        "id": "tH7C1UwBJJgU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import coint, adfuller"
      ],
      "metadata": {
        "id": "Pk8u-TExE8FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Emissions = ts.iloc[:,1:]"
      ],
      "metadata": {
        "id": "gbm4VMEgL5aI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Monthly total emissions (mte)"
      ],
      "metadata": {
        "id": "zG6mHWgmL8hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Emissions = ts.iloc[:,1:]   # Monthly total emissions (mte)\n",
        "Emissions= Emissions.groupby(['Description', pd.TimeGrouper('M')])['Value'].sum().unstack(level = 0)\n",
        "mte = Emissions['Natural Gas Electric Power Sector CO2 Emissions'] # monthly total emissions (mte)\n",
        "mte.head()"
      ],
      "metadata": {
        "id": "cSUy48GgTKPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mte.tail()"
      ],
      "metadata": {
        "id": "e170qsT1TNH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.tsa.stattools import coint, adfuller"
      ],
      "metadata": {
        "id": "_iOR2aXoKAWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(mte)"
      ],
      "metadata": {
        "id": "3bzsOX4_TVR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test stationary using Dickey-Fuller"
      ],
      "metadata": {
        "id": "xnmNtx6gTas3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TestStationaryPlot(ts):\n",
        "    rol_mean = ts.rolling(window = 12, center = False).mean()\n",
        "    rol_std = ts.rolling(window = 12, center = False).std()\n",
        "    \n",
        "    plt.plot(ts, color = 'blue',label = 'Original Data')\n",
        "    plt.plot(rol_mean, color = 'red', label = 'Rolling Mean')\n",
        "    plt.plot(rol_std, color ='black', label = 'Rolling Std')\n",
        "    plt.xticks(fontsize = 25)\n",
        "    plt.yticks(fontsize = 25)\n",
        "    \n",
        "    plt.xlabel('Time in Years', fontsize = 25)\n",
        "    plt.ylabel('Total Emissions', fontsize = 25)\n",
        "    plt.legend(loc='best', fontsize = 25)\n",
        "    plt.title('Rolling Mean & Standard Deviation', fontsize = 25)\n",
        "    plt.show(block= True)"
      ],
      "metadata": {
        "id": "qXlgMBLBThtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestStationaryPlot(mte)"
      ],
      "metadata": {
        "id": "zuLuA58FTxA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def TestStationaryAdfuller(ts, cutoff = 0.01):\n",
        "\n",
        "  ts_test = adfuller(ts, autolag = 'AIC')\n",
        "  ts_test_output = pd.Series(ts_test[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        " \n",
        "for key,value in ts_test[4].items():\n",
        "\n",
        "  ts_test_output['Critical Value (%s)'%key] = value\n",
        "  print(ts_test_output)\n",
        "    \n",
        "  if ts_test[1] <= cutoff:\n",
        "\n",
        "    print(\"Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary\")\n",
        "  else:\n",
        "    print(\"Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")"
      ],
      "metadata": {
        "id": "0ctpuz2Z1ek-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestStationaryAdfuller(mte)"
      ],
      "metadata": {
        "id": "jRoO7CAtvyYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving average"
      ],
      "metadata": {
        "id": "c7PoXFhN1DPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this technique, we take average of ‘k’ consecutive values depending on the frequency of time series (in this case 12 monthes per year). Here, we will take the average over the past 1 year."
      ],
      "metadata": {
        "id": "c5o_Ysi61knm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "moving_avg = mte.rolling(12).mean()\n",
        "plt.plot(mte)\n",
        "plt.plot(moving_avg, color='red')\n",
        "plt.xticks(fontsize = 25)\n",
        "plt.yticks(fontsize = 25)\n",
        "plt.xlabel('Time (years)', fontsize = 25)\n",
        "plt.ylabel('CO2 Emission (MMT)', fontsize = 25)\n",
        "plt.title('CO2 emission from electric power generation', fontsize = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dymuf4gD2Gvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mte_moving_avg_diff = mte - moving_avg\n",
        "mte_moving_avg_diff.head(13)"
      ],
      "metadata": {
        "id": "5Ql4xLUM2wvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mte_moving_avg_diff.dropna(inplace=True)\n",
        "TestStationaryPlot(mte_moving_avg_diff)"
      ],
      "metadata": {
        "id": "55mlCqk221J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestStationaryAdfuller(mte_moving_avg_diff)"
      ],
      "metadata": {
        "id": "in3d8-zU24GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary"
      ],
      "metadata": {
        "id": "jiVDJKll29KH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The rolling mean values appear to be varying slightly. The Test Statistic is smaller than the 10% 5%, and 1% of critical values. So, we can say with 99% confidence level that the dataset is a stationary series."
      ],
      "metadata": {
        "id": "PhdwYQAP3AxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exponentail weighted moving average"
      ],
      "metadata": {
        "id": "9KYmho6w3M6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another technique is to take the ‘weighted moving average’ where more recent values are given a higher weight. The popular method to assign the waights is using the exponential weighted moving average. Where weights are assigned to all previous values with a decay factor."
      ],
      "metadata": {
        "id": "UvqbFZ7N3Txs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mte_exp_wighted_avg = pd.ewma(mte, halflife=12)\n",
        "plt.plot(mte)\n",
        "plt.plot(mte_exp_wighted_avg, color='red')\n",
        "plt.xticks(fontsize = 25)\n",
        "plt.yticks(fontsize = 25)\n",
        "plt.xlabel('Time (years)', fontsize = 25)\n",
        "plt.ylabel('CO2 Emission (MMT)', fontsize = 25)\n",
        "plt.title('CO2 emission from electric power generation', fontsize = 25)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "K8Epq3Q93XaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mte_ewma_diff = mte - mte_exp_wighted_avg\n",
        "TestStationaryPlot(mte_ewma_diff)"
      ],
      "metadata": {
        "id": "4YWubxcZ3ana"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestStationaryAdfuller(mte_ewma_diff)"
      ],
      "metadata": {
        "id": "uIoMwZjs3dtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary "
      ],
      "metadata": {
        "id": "vhzxMRts3gc3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This time series has lesser variations in mean and standard deviation compared to the orginal ddataset. Also, the Test Statistic is smaller than the 5% and 10% critical value, which is better than the original case. There will be no missing values as all values from starting are given weights. So, it will work even with no previous values. In this case, we can say with 95% confidence level the series is a stationary series."
      ],
      "metadata": {
        "id": "ec1seu9J3jSR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eliminating trend and seasonality: Differencing"
      ],
      "metadata": {
        "id": "yQZvsnv23mzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the most common method of dealing with both trend and seasonality is differencing. In this technique, we take the difference of the original observation at a particular instant with that at the previous instant. This mostly works well to improve stationarity. First order differencing can be done as follows:"
      ],
      "metadata": {
        "id": "8KcgbJ_t3tew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mte_first_difference = mte - mte.shift(1)  \n",
        "TestStationaryPlot(mte_first_difference.dropna(inplace=False))"
      ],
      "metadata": {
        "id": "o7lAf3Wz4BQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestStationaryAdfuller(mte_first_difference.dropna(inplace=False))"
      ],
      "metadata": {
        "id": "J5UKKVE53yUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary"
      ],
      "metadata": {
        "id": "j8upK9wI31L1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first difference improves the stationarity of the series significantly. Let us use also the seasonal difference to remove the seasonality of the data and see how that impacts stationarity of the data."
      ],
      "metadata": {
        "id": "pIpAFKUX4Ijb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mte_seasonal_difference = mte - mte.shift(12)  \n",
        "TestStationaryPlot(mte_seasonal_difference.dropna(inplace=False))\n",
        "TestStationaryAdfuller(mte_seasonal_difference.dropna(inplace=False))"
      ],
      "metadata": {
        "id": "nFul8Wu84LKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary"
      ],
      "metadata": {
        "id": "ZF220HMX4OyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compared to the original data the seasonal difference also improves the stationarity of the series. The next step is to take the first difference of the seasonal difference."
      ],
      "metadata": {
        "id": "DiGz7saz4RHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mte_seasonal_first_difference = mte_first_difference - mte_first_difference.shift(12)  \n",
        "TestStationaryPlot(mte_seasonal_first_difference.dropna(inplace=False))"
      ],
      "metadata": {
        "id": "oa0MML1i4UId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestStationaryAdfuller(mte_seasonal_first_difference.dropna(inplace=False))"
      ],
      "metadata": {
        "id": "IqHc0_6c3OrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary"
      ],
      "metadata": {
        "id": "oA_2lHJn4vYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we look the Test Statistic and the p-value, taking the seasonal first difference has made our the time series dataset stationary. This differencing procedure could be repeated for the log values, but it didn’t make the dataset any more stationary."
      ],
      "metadata": {
        "id": "ZhoQI8IK4xmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mte_seasonal_first_difference = mte_first_difference - mte_first_difference.shift(12)  \n",
        "TestStationaryPlot(mte_seasonal_first_difference.dropna(inplace=False))"
      ],
      "metadata": {
        "id": "ikhUnd5I8gk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TestStationaryAdfuller(mte_seasonal_first_difference.dropna(inplace=False))"
      ],
      "metadata": {
        "id": "-lrsHnJY8r3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary"
      ],
      "metadata": {
        "id": "zQdUzzfi8vh_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, if we look the Test Statistic and the p-value, taking the seasonal first difference has made our the time series dataset stationary. This differencing procedure could be repeated for the log values, but it didn’t make the dataset any more stationary."
      ],
      "metadata": {
        "id": "ZC2awhv18zJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eliminating trend and seasonality: Decomposing"
      ],
      "metadata": {
        "id": "01DkTV3u83Hb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this technique, it statrating by modeling both trend and seasonality and removing them from the model."
      ],
      "metadata": {
        "id": "_XepfXOy9TuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "decomposition = seasonal_decompose(mte)\n",
        "\n",
        "trend = decomposition.trend\n",
        "seasonal = decomposition.seasonal\n",
        "residual = decomposition.resid\n",
        "\n",
        "plt.subplot(411)\n",
        "plt.plot(mte, label='Original')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(412)\n",
        "plt.plot(trend, label='Trend')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(413)\n",
        "plt.plot(seasonal,label='Seasonality')\n",
        "plt.legend(loc='best')\n",
        "plt.subplot(414)\n",
        "plt.plot(residual, label='Residuals')\n",
        "plt.legend(loc='best')\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "XAv2jgPP9aJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see that the trend, seasonality are separated out from data and we can model the residuals. Lets check stationarity of residuals:"
      ],
      "metadata": {
        "id": "8k800ue49mue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mte_decompose = residual\n",
        "mte_decompose.dropna(inplace=True)\n",
        "TestStationaryPlot(mte_decompose)\n",
        "TestStationaryAdfuller(mte_decompose)"
      ],
      "metadata": {
        "id": "1N2x1QS09pFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary"
      ],
      "metadata": {
        "id": "KfHXRSB49sM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Find optimal parameters and build SARIMA model**"
      ],
      "metadata": {
        "id": "-TI5-vT79vhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When looking to fit time series dataset with seasonal **ARIMA** model, our first goal is to find the values of **SARIMA**(p,d,q)(P,D,Q)s that optimize our metric of interest. Before moving directly how to find the optimal values of the parameters let us see the two situations in stationarities:"
      ],
      "metadata": {
        "id": "sTBoDn5i9-Ms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the easy case wherein we can model the residuals as **white noise**. The second case being a series with significant dependence among values and needs statistical models like** ARIMA to forecast future oucomes.**"
      ],
      "metadata": {
        "id": "mGx6AFrp95NN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Auto-Regressive Integrated Moving Average (ARIMA):** The ARIMA forecasting for a stationary time series is a linear funcion similar to linear regression. The predictors mainly depend on the parameters (p,d,q) of the ARIMA model:"
      ],
      "metadata": {
        "id": "qBYlF_LU-FwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Plot the ACF and PACF charts and find the optimal parameters"
      ],
      "metadata": {
        "id": "c2q39Tc5-X_w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**- Autocorrelation Function (ACF)**: It is a measure of the correlation between the the time series (ts) with a lagged version of itself. For instance at lag 4, ACF would compare series at time instant ‘t1’…’t2’ with series at instant ‘t1-4’…’t2-4’ (t1-4 and t2 being end points of the range).\n",
        "\n",
        "\n",
        "**- Partial Autocorrelation Function (PACF):** This measures the correlation between the ts with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 4, it will check the correlation but remove the effects already explained by lags 1 to 3."
      ],
      "metadata": {
        "id": "RQ3iHChY-ccT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Therefore, the next step will be determing the **tuning parameters** (p and q) of the model by looking at the a**utocorrelation and partial autocorrelation graphs**. The chart below provides a brief guide on how to read the autocorrelation and partial autocorrelation graphs inorder to select the parameters."
      ],
      "metadata": {
        "id": "Znft4GEO-qht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(12,8))\n",
        "ax1 = fig.add_subplot(211)\n",
        "fig = sm.graphics.tsa.plot_acf(mte_seasonal_first_difference.iloc[13:], lags=40, ax=ax1)\n",
        "ax2 = fig.add_subplot(212)\n",
        "fig = sm.graphics.tsa.plot_pacf(mte_seasonal_first_difference.iloc[13:], lags=40, ax=ax2)"
      ],
      "metadata": {
        "id": "UjpzIOkU-ZKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid search**"
      ],
      "metadata": {
        "id": "g3j1sS1U-3uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the optimal parameters for **ARIMA models** using the **graphical method is not trivial** and it is time consuming. We will select the optimal parameter values systematically using the grid search (**hyperparameter optimization**) method. The grid search iteratively explore different combinations of the parameters. For each combination of parameters, we will fit a new seasonal ARIMA model with the **SARIMAX()** function from the statsmodels module and assess its overall quality. Once we have explored the entire landscape of parameters, our optimal set of parameters will be the one that yields the best performance for our criteria of interest. Let's begin by generating the various combination of parameters that we wish to assess:"
      ],
      "metadata": {
        "id": "pOZV8UTp-_xH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p = d = q = range(0, 2) # Define the p, d and q parameters to take any value between 0 and 2\n",
        "pdq = list(itertools.product(p, d, q)) # Generate all different combinations of p, q and q triplets\n",
        "pdq_x_QDQs = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))] # Generate all different combinations of seasonal p, q and q triplets\n",
        "print('Examples of Seasonal ARIMA parameter combinations for Seasonal ARIMA...')\n",
        "print('SARIMAX: {} x {}'.format(pdq[1], pdq_x_QDQs[1]))\n",
        "print('SARIMAX: {} x {}'.format(pdq[2], pdq_x_QDQs[2]))"
      ],
      "metadata": {
        "id": "iqKMiEV2_SBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pdq)\n",
        "print(pdq_x_QDQs)"
      ],
      "metadata": {
        "id": "tl5YKPEj_Uvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When evaluating and comparing statistical models fitted with different parameters, each can be ranked against one another based on how well it fits the data or its ability to accurately predict future data points. We will use the **AIC** (Akaike Information Criterion) value, which is conveniently returned with **ARIMA models** fitted using **statsmodels**. The **AIC** **measures how well a model fits the data** while taking into account the overall complexity of the model. A model that fits the data very well while using lots of features will be assigned a **larger AIC score than a model** that uses fewer features to achieve the same **goodness-of-fit**. The lowest AIC refore, we are interested in finding the model that yields the lowest AIC value."
      ],
      "metadata": {
        "id": "PYuPrXyW_beY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The order argument specifies the (p, d, q) parameters, while the seasonal_order argument specifies the (P, D, Q, S) seasonal component of the Seasonal ARIMA model. After fitting each SARIMAX()model, the code prints out its respective AIC score."
      ],
      "metadata": {
        "id": "EBEH3Rqm_-Oo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notes on AIC score: AIC will choose the best model from a set(**The “best” model will be the one that neither under-fits nor over-fits**.),then consider running a hypothesis test to figure out the relationship between the variables in your model and the outcome of interest."
      ],
      "metadata": {
        "id": "i0f35zAOAE7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for param in pdq:\n",
        "    for seasonal_param in pdq_x_QDQs:\n",
        "        try:\n",
        "            mod = sm.tsa.statespace.SARIMAX(mte,\n",
        "                                            order=param,\n",
        "                                            seasonal_order=seasonal_param,\n",
        "                                            enforce_stationarity=False,\n",
        "                                            enforce_invertibility=False)\n",
        "            results = mod.fit()\n",
        "            print('ARIMA{}x{} - AIC:{}'.format(param, param_seasonal, results.aic))\n",
        "        except:\n",
        "          continue"
      ],
      "metadata": {
        "id": "3h73MPrvAHf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=[]\n",
        "b=[]\n",
        "c=[]\n",
        "wf=pd.DataFrame()"
      ],
      "metadata": {
        "id": "mfuxbBcgAWvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
        "\n",
        "for param in pdq:\n",
        "    for param_seasonal in pdq_x_QDQs:\n",
        "        try:\n",
        "            mod = sm.tsa.statespace.SARIMAX(mte,\n",
        "                                            order=param,\n",
        "                                            seasonal_order=param_seasonal,\n",
        "                                            enforce_stationarity=False,\n",
        "                                            enforce_invertibility=False)\n",
        "            results = mod.fit()\n",
        "\n",
        "            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
        "            a.append(param)\n",
        "            b.append(param_seasonal)\n",
        "            c.append(results.aic)\n",
        "        except:\n",
        "            continue\n",
        "wf['pdq']=a\n",
        "wf['pdq_x_QDQs']=b\n",
        "wf['aic']=c\n",
        "print(wf[wf['aic']==wf['aic'].min()])"
      ],
      "metadata": {
        "id": "Sl5W08fhAaLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SARIMAX(1, 1, 1)x(0, 1, 1, 12) yields the lowest AIC value of 2003.553. Therefore, we will consider this to be optimal option out of all the parameter combinations.W e have identified the set of parameters that produces the best fitting model to our time series data. We can proceed to analyze this particular model in more depth"
      ],
      "metadata": {
        "id": "Q-J1M0zeAm07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod = sm.tsa.statespace.SARIMAX(mte, \n",
        "                                order=(1,1,1), \n",
        "                                seasonal_order=(0,1,1,12),   \n",
        "                                enforce_stationarity=False,\n",
        "                                enforce_invertibility=False)\n",
        "results = mod.fit()\n",
        "print(results.summary())"
      ],
      "metadata": {
        "id": "qxyj-b5JAqSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The coef column shows the weight (i.e. importance) of each feature and how each one impacts the time series. The P>|z| column informs us of the significance of each feature weight. Here, each weight has a p-value close to 0, so it is reasonable to include the features in our model.\n",
        "\n",
        "When fitting seasonal ARIMA models, it is important to run model diagnostics to ensure that none of the assumptions made by the model have been violated. First, we get a line plot of the residual errors, suggesting that there may still be some trend information not captured by the model."
      ],
      "metadata": {
        "id": "NvC9wTlsAume"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.resid.plot(figsize=(12,8))"
      ],
      "metadata": {
        "id": "HQSLBp4xAw-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(results.resid.describe())"
      ],
      "metadata": {
        "id": "tPvYfSnTAzy1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The figure displays the distribution of the residual errors. It shows a little bias in the prediction. Next, we get a density plot of the residual error values, suggesting the errors are Gaussian, but may not be centered on zero."
      ],
      "metadata": {
        "id": "j8Zs7RNOA2WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.resid.plot(figsize=(12,8),kind='kde')"
      ],
      "metadata": {
        "id": "4mvmVPDdA4dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot_diagnostics object allows us to quickly generate model diagnostics and investigate for any unusual behavior."
      ],
      "metadata": {
        "id": "YUaCeof4A7BH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results.plot_diagnostics(figsize=(15, 12))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LI4IUQoyA9YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our primary concern is to ensure that the residuals of our model are uncorrelated and normally distributed with zero-mean. If the seasonal ARIMA model does not satisfy these properties, it is a good indication that it can be further improved."
      ],
      "metadata": {
        "id": "zdIY3WZ5BCoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model diagnostic suggests that the model residual is normally distributed based on the following::\n",
        "\n",
        "- In the top right plot, the red KDE line follows closely with the N(0,1) line. Where, N(0,1) is the standard notation for a normal distribution with mean 0 and standard deviation of 1. \n",
        "\n",
        "- This is a good indication that the residuals are normally distributed. The forecast errors deviate somewhat from the straight line, indicating that the normal distribution is not a perfect model for the distribution of forecast errors, but it is not unreasonable.\n",
        "\n",
        "- The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) follows the linear trend of the samples taken from a standard normal distribution. Again, this is a strong indication that the residuals are normally distributed.\n",
        "\n",
        "- The residuals over time (top left plot) don't display any obvious seasonality and appear to be white noise. This is confirmed by the autocorrelation (i.e. correlogram) plot on the bottom right,which shows that the time series residuals have low correlation with lagged versions of itself.\n",
        "\n",
        "- Those observations lead us to conclude that our model produces a satisfactory fit that could help us understand our time series data and forecast future values."
      ],
      "metadata": {
        "id": "aamCmKFiBH72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Validating prediction**"
      ],
      "metadata": {
        "id": "yXDidFXvBhmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have obtained a model for our time series that can now be used to produce **forecasts**. We start by comparing predicted values to real values of the time series, which will help us understand the accuracy of our forecast. The get_prediction() and conf_int() attributes allow us to obtain the values and associated confidence intervals for **forecasts of the time series.**"
      ],
      "metadata": {
        "id": "GELGfe-4Bnrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = results.get_prediction(start = 480, end = 523, dynamic=False)\n",
        "pred_ci = pred.conf_int()\n",
        "pred_ci.head()"
      ],
      "metadata": {
        "id": "2U3kqU_OBuHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dynamic=False argument ensures that we produce one-step ahead forecasts, meaning that forecasts at each point are generated using the full history up to that point.\n",
        "\n",
        "We can plot the real and forecasted values of the CO2 emission time series to assess how well the model fits."
      ],
      "metadata": {
        "id": "HL9WpsHlB33L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = mte['1973':].plot(label='observed')\n",
        "pred.predicted_mean.plot(ax=ax, label='One-step ahead forecast', alpha=.7)\n",
        "\n",
        "ax.fill_between(pred_ci.index,\n",
        "                pred_ci.iloc[:, 0],\n",
        "                pred_ci.iloc[:, 1], color='r', alpha=.5)\n",
        "\n",
        "ax.set_xlabel('Time (years)')\n",
        "ax.set_ylabel('NG CO2 Emissions')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iNEqRi4rB7Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, our forecasts align with the true values very well, showing an overall similar behavior.\n",
        "\n",
        "It is also useful to quantify the accuracy of our forecasts. We will use the MSE (Mean Squared Error), which summarizes the average error of our forecasts. For each predicted value, we compute its distance to the true value and square the result. The results need to be squared so that positive/negative differences do not cancel each other out."
      ],
      "metadata": {
        "id": "cSQMvddyCFAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mte_forecast = pred.predicted_mean\n",
        "mte_truth = mte['2013-01-31':]\n",
        "\n",
        "# Compute the mean square error\n",
        "mse = ((mte_forecast - mte_truth) ** 2).mean()\n",
        "print('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))\n",
        "print('The Root Mean Square Error (RMSE) of the forcast: {:.4f}'\n",
        "      .format(np.sqrt(sum((mte_forecast-mte_truth)**2)/len(mte_forecast))))"
      ],
      "metadata": {
        "id": "xT9QgvfzCJyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Mean Squared Error (MSE) of the forecast is 4.09\n",
        "The Root Mean Square Error (RMSE) of the forcast: nan"
      ],
      "metadata": {
        "id": "74QCSjdFCMhu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mte_pred_concat = pd.concat([mte_truth, mte_forecast])"
      ],
      "metadata": {
        "id": "sBPoSiscCPCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of developing the model is to get a good quality predictive power using dynamic forecast. That is, we use information from the time series up to a certain point, and after that, forecasts are generated using values from previous forecasted time points as follows:"
      ],
      "metadata": {
        "id": "p_Q_0d_jCT9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_dynamic = results.get_prediction(start=pd.to_datetime('2013-01-31'), dynamic=True, full_results=True)\n",
        "pred_dynamic_ci = pred_dynamic.conf_int()"
      ],
      "metadata": {
        "id": "us8_FBarBcrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From, plotting the observed and forecasted values of the time series, we see that the overall forecasts are accurate even when we use the dynamic forecast. All forecasted values (red line) match closely to the orginal observed (blue line) data, and are well within the confidence intervals of our forecast."
      ],
      "metadata": {
        "id": "I1-SGQUwCadu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = mte['1973':].plot(label='observed', figsize=(20, 15))\n",
        "pred_dynamic.predicted_mean.plot(label='Dynamic Forecast', ax=ax)\n",
        "\n",
        "ax.fill_between(pred_dynamic_ci.index,\n",
        "                pred_dynamic_ci.iloc[:, 0],\n",
        "                pred_dynamic_ci.iloc[:, 1], \n",
        "                color='r', \n",
        "                alpha=.3)\n",
        "\n",
        "ax.fill_betweenx(ax.get_ylim(), \n",
        "                 pd.to_datetime('2013-01-31'), \n",
        "                 mte.index[-1],\n",
        "                 alpha=.1, zorder=-1)\n",
        "\n",
        "ax.set_xlabel('Time (years)')\n",
        "ax.set_ylabel('CO2 Emissions')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rvqyNzDNCdy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the predicted and true values of our time series\n",
        "mte_forecast = pred_dynamic.predicted_mean\n",
        "mte_orginal = mte['2013-01-31':]\n",
        "\n",
        "# Compute the mean square error\n",
        "mse = ((mte_forecast - mte_orginal) ** 2).mean()\n",
        "print('The Mean Squared Error (MSE) of the forecast is {}'.format(round(mse, 2)))\n",
        "print('The Root Mean Square Error (RMSE) of the forcast: {:.4f}'\n",
        "      .format(np.sqrt(sum((mte_forecast-mte_orginal)**2)/len(mte_forecast))))"
      ],
      "metadata": {
        "id": "c9UY3RUXCjwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Forecasting**"
      ],
      "metadata": {
        "id": "dOb3-yBRCnnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get forecast of 10 years or 120 months steps ahead in future\n",
        "forecast = results.get_forecast(steps= 120)\n",
        "# Get confidence intervals of forecasts\n",
        "forecast_ci = forecast.conf_int()\n",
        "forecast_ci.head()"
      ],
      "metadata": {
        "id": "HsBxw8AwCpJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ax = mte.plot(label='observed', figsize=(20, 15))\n",
        "forecast.predicted_mean.plot(ax=ax, label='Forecast')\n",
        "ax.fill_between(forecast_ci.index,\n",
        "                forecast_ci.iloc[:, 0],\n",
        "                forecast_ci.iloc[:, 1], color='g', alpha=.4)\n",
        "ax.set_xlabel('Time (year)')\n",
        "ax.set_ylabel('NG CO2 Emission level')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BNHG98bSCuKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the forecast and associated confidence interval that we have generated can now be used to further explore and understand the time series. The forecast shows that the CO2 emission from natural gas power generation is expected to continue increasing."
      ],
      "metadata": {
        "id": "1WE5gMolCxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**"
      ],
      "metadata": {
        "id": "6VqR7hevCzuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I have explored how to retrieve CSV dataset, how to transform the dataset into times series, testing if the time series is stationary or not using graphical and Dickey-Fuller test statistic methods, how to transform time series to stationary, how to find optimal parameters to build SARIMA model using grid search method, diagnosing time series prediction, validating the predictive power, forecasting 10 year future CO2 emission from power generation using natural gas.\n",
        "\n",
        "Future work: developing a time series model of natural gas forecasing"
      ],
      "metadata": {
        "id": "BFyFo7AVC3m4"
      }
    }
  ]
}